{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Loading tavily stuffs\n",
    "from langchain.tools.tavily_search import TavilySearchResults \n",
    "\n",
    "### Loading langchain stuffs\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "### some more shitty stuffs\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # avoid warning messages importing packages\n",
    "\n",
    "import json\n",
    "\n",
    "REQUEST_PER_QUESTION = 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f499cce6cc0f06f5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Tavily API Key\n",
    "auth_key_tavily = os.environ.get(\"TAVILY_API_KEY\")\n",
    "\n",
    "# HuggingFace API Key\n",
    "auth_key_hf = os.environ.get(\"HF_KEY\")\n",
    "\n",
    "# Langchain key\n",
    "langchain_key = os.environ.get(\"LANGCHAIN_API_KEY\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d916759a5c24e1bc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "# snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir='./Mistral_7B_Instruct_v0.3/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "691441f2e6348212",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, pipeline\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "mistral_models_path = './Mistral_7B_Instruct_v0.3/' \n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "# model = Transformer.from_file({mistral_models_path})\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', quantization_config=quantization_config)\n",
    "\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "generated_ids = model.generate(torch.tensor([tokens]).to('cuda'), max_new_tokens=128)\n",
    "result = tokenizer.decode(generated_ids[0].tolist())\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bcfe5623ad9b42f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "# \n",
    "# encodeds = tokenizer_auto.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "# \n",
    "# generated_ids = model.generate(encodeds.to('cuda'), max_new_tokens=64, do_sample=True)\n",
    "# decoded = tokenizer_auto.batch_decode(generated_ids)\n",
    "# print(decoded[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a160e540bb86bb58",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_huggingface import (\n",
    "    HuggingFacePipeline, HuggingFaceEmbeddings, ChatHuggingFace, HuggingFaceEndpoint\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3', max_length=128, skip_special_tokens=True)\n",
    "tokenizer_auto.pad_token = tokenizer_auto.eos_token\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer_auto, \n",
    "    do_sample=True, \n",
    "    max_new_tokens=128,\n",
    "    truncation='only_first',\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    ")\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     endpoint_url = \"https://api-inference.huggingface.co/models/google/gemma-2b-it\",\n",
    "#     task='text-generation',\n",
    "#     max_new_tokens=64,\n",
    "#     top_k=10,\n",
    "#     top_p=0.9,\n",
    "#     repetition_penalty=1.03,\n",
    "#     timeout=240,\n",
    "#     huggingfacehub_api_token=auth_key_hf\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d522411dab09ff9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm('<s><INST>Your task is to make a ordered list of the items</INST> for items=(banana, apple), your output: 1. apple, 2. banana.</s><INST> Make list for items \"eggs, bacon, toast\"</INST>')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dc51c2fd94a007f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4373eb1442a6232",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "def web_search(query:str, num_results:int=2):\n",
    "    results = DDGS().text(query, max_results=num_results)\n",
    "    return [r[\"href\"] for r in results] \n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the webpage using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Extract the text from the webpage\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "            return text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage. \\\n",
    "                    Status code: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83b1fb194b3e6bc7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "web_search(\"What is the impact of machine learning on audio faking?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5f2a86c1d4fc8e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# summary_template = f\"use the {text} to summarize the question: {question}\"\n",
    "\n",
    "# def summary_template(question:str, text:str):\n",
    "#     return f\"\"\"<s><INST>Answer the question:\"{question}\" by summarizing the text: \"{text}\" if the question cannot be answered using the text.</INST>\"\"\"\n",
    "\n",
    "# summary_template = \"\"\"<s><INST>Answer the question:\"{question}\" by summarizing the text: \"{text}\" if the question cannot be answered using the text.</INST>\"\"\"\n",
    "# SUMMARY_PROMPT = ChatPromptTemplate.from_template(summary_template)\n",
    "\n",
    "SUMMARY_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful AI bot.'),\n",
    "    ('user', 'Answer the question:\"{question}\" by summarizing the text: \"{text}\". If the question cannot be answered using the text, return the text.' )\n",
    "])\n",
    "\n",
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    summary=RunnablePassthrough.assign(\n",
    "        text=lambda x: scrape_text(x[\"url\"])[:10]\n",
    "    ) | SUMMARY_PROMPT | llm | StrOutputParser()\n",
    ") | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {x['summary']}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52b9c038dbf4f259",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "web_search_chain = (RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"])\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) \n",
    "  | scrape_and_summarize_chain.map())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf0f78ca92d4fddb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "web_search_chain.invoke({\"question\": \"What is the impact of machine learning on audio faking?\"})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b422c2b3c1975c12",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import regex as re"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15c576772ba6dbb0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "search_template = \"\"\"\n",
    "        <s>\n",
    "        <INST>\n",
    "            Write only 2 google search queries to search from the following question: \"{question}\". \n",
    "            Return a numbered list of the queries.\n",
    "        </INST>\n",
    "    \"\"\"\n",
    "\n",
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    ('user', 'Write only 2 google search queries to search from the following question: \"{question}\". Return a numbered list of the queries.' ),   \n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff8e676a17f23ce7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "search_question_chain = (SEARCH_PROMPT | llm | StrOutputParser() | (lambda x: re.findall(r'[1-9]\\.\\s?\"(.*)\"\\n?', x)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c38a4f6cadc32af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "search_qs_test = search_question_chain.invoke({'question': 'What is the impact of machine learning on audio faking?'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb0f2edcc7fdfe28",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "search_qs_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f23e5fbe1254ff3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_research_chain = search_question_chain |  (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2840a96c49da4f8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_research_chain.invoke({'question': 'What is the impact of machine learning on audio faking?'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fce60079ffbb55f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35bf00471993918d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "WRITER_SYSTEM_PROMPT = \"\"\"\n",
    "        <s>\n",
    "        <INST>\n",
    "            You are an AI critical thinker research assistant. \n",
    "            Your sole purpose is to write \n",
    "            well written, critically acclaimed, \n",
    "            objective and structured reports on given text or task. \n",
    "        </INST>\n",
    "    \"\"\"\n",
    "\n",
    "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
    "RESEARCH_REPORT_TEMPLATE = \"\"\"<s><INST>\n",
    "    Using the information \"{research_summary}\", answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
    "You must write the report with markdown syntax.\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
    "You must write the report in apa format.\n",
    "Please do your best, this is very important to my career.</INST>\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fedf211fa13e49ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def collapse_list_of_lists(list_of_lists):\n",
    "    content = []\n",
    "    for l in list_of_lists:\n",
    "        content.append(\"\\n\\n\".join(l))\n",
    "    return \"\\n\\n\".join(content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e27caa18515927",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain = (RunnablePassthrough.assign(\n",
    "    research_summary=full_research_chain | collapse_list_of_lists\n",
    ") | prompt | llm | StrOutputParser())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58f124640e235dcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain.invoke({\"question\":'what is the impact of machine learning on audio faking'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "261ac67559e24b11",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
