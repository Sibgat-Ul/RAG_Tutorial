{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "TAVILY_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "HF_KEY = os.getenv(\"HF_API_KEY\")\n",
    "LC_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "WEAVIATE_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T15:34:29.819112Z",
     "start_time": "2024-07-08T15:34:29.812521Z"
    }
   },
   "id": "f995d1c552cc0986",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from tavily import TavilyClient\n",
    "from duckduckgo_search import AsyncDDGS, DDGS\n",
    "\n",
    "SEARCH_ENGINE = Literal[\"tavily\", \"ddgs\", \"both\"]\n",
    "\n",
    "def search_ddgs(query:str, max_results:int=2) -> list:\n",
    "    ddgs = DDGS()\n",
    "    results = None\n",
    "    \n",
    "    try:\n",
    "        results = ddgs.text(keywords=query, max_results=max_results)\n",
    "        results = [{'url': result['href'], 'title': result['title'], 'content': result['body']} for result in results]\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print('Duck Duck Go search is unavailable now, using tavily')\n",
    "        results = search_tavily(query, max_results)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def search_tavily(query:str, max_results:int=2) -> list:\n",
    "    results = None\n",
    "    \n",
    "    try:\n",
    "        tavily_client = TavilyClient(api_key=TAVILY_KEY)\n",
    "        results = tavily_client.search(query=query, max_results=max_results, search_depth='basic')['results']\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        \n",
    "    return results\n",
    "\n",
    "def search_web(query:str, search_engine:SEARCH_ENGINE, max_results:int=2) -> dict:\n",
    "    results = {} \n",
    "    \n",
    "    assert search_engine in [\"tavily\", \"ddgs\", \"both\"], \"Invalid search engine\"\n",
    "     \n",
    "    if search_engine == \"tavily\":\n",
    "        results['results'] = search_tavily(query, max_results)\n",
    "        \n",
    "    elif search_engine == \"ddgs\":\n",
    "        results['results'] = search_ddgs(query, max_results)\n",
    "        \n",
    "    elif search_engine == \"both\":\n",
    "        results['results'] = [*search_tavily(query, max_results), *search_ddgs(query, max_results)]\n",
    "     \n",
    "    return results "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T15:34:30.226377Z",
     "start_time": "2024-07-08T15:34:30.221416Z"
    }
   },
   "id": "144edea260cde3f4",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def collapse_results(results:dict) -> list:\n",
    "    return ['title:' + result['title'] + '\\nContent' + result['content'] for result in results['results']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T15:34:30.616665Z",
     "start_time": "2024-07-08T15:34:30.614636Z"
    }
   },
   "id": "18850089e2ec8f22",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vector DB: Weaviate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe96ebb99731ae77"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "vect_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T15:34:36.515055Z",
     "start_time": "2024-07-08T15:34:31.357056Z"
    }
   },
   "id": "56ba53c325129e34",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from weaviate.auth import AuthApiKey\n",
    "\n",
    "def search_and_create_embedding(query:str):\n",
    "    client = None\n",
    "\n",
    "    try:\n",
    "        client = weaviate.connect_to_wcs(\n",
    "            WEAVIATE_URL, \n",
    "            auth_credentials=AuthApiKey(WEAVIATE_KEY),\n",
    "            additional_config=wvc.init.AdditionalConfig(timeout=wvc.init.Timeout(init=360)),\n",
    "            skip_init_checks=True\n",
    "        )\n",
    "        print(client.schema.get())\n",
    "        if client.is_ready():\n",
    "            print(\"Connection to Weaviate established\")\n",
    "            research_docs = None\n",
    "            \n",
    "            if not client.collections.exists(\"ResearchDocs\"):\n",
    "                research_docs = client.collections.create(\n",
    "                    name='ResearchDocs',\n",
    "                    vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n",
    "                )\n",
    "            \n",
    "            else: \n",
    "                research_docs = client.collections.get('ResearchDocs')\n",
    "                \n",
    "                results = search_web(query=query, search_engine=\"both\", max_results=2)\n",
    "                print(\"Search results:\", len(results['results']))\n",
    "\n",
    "                docs = collapse_results(results)\n",
    "                print(\"Docs:\", len(docs))\n",
    "                emb = (vect_model.encode(docs)).tolist()\n",
    "\n",
    "                wvc_data_objects = list()\n",
    "                for i, (d, e) in enumerate(zip(docs, emb)):\n",
    "                    wvc_data_objects.append(\n",
    "                        wvc.data.DataObject(\n",
    "                            properties={\n",
    "                                \"title\": results['results'][i]['title'],\n",
    "                                \"content\": results['results'][i]['content'],\n",
    "                            },\n",
    "                            vector=e\n",
    "                        )\n",
    "                    )\n",
    "                research_docs.data.insert_many(wvc_data_objects)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    finally:\n",
    "        client.close()\n",
    "\n",
    "def search_weaviate(query:str):\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        client = weaviate.connect_to_wcs(WEAVIATE_URL, auth_credentials=AuthApiKey(WEAVIATE_KEY))\n",
    "\n",
    "        if client.is_ready():\n",
    "            print(\"Connection to Weaviate established\")\n",
    "\n",
    "            if client.collections.exists(\"ResearchDocs\"):\n",
    "                research_docs = client.collections.get(\"ResearchDocs\")\n",
    "                query_vector = (vect_model.encode(query)).tolist()\n",
    "\n",
    "                results = research_docs.query.near_vector(\n",
    "                    near_vector=query_vector,\n",
    "                    limit=2,\n",
    "                    return_metadata=wvc.query.MetadataQuery(certainty=True)\n",
    "                )\n",
    "                client.close()\n",
    "                return results.objects[1].properties\n",
    "\n",
    "            client.close()\n",
    "        else:\n",
    "            raise Exception(\"Weaviate connection failed\")\n",
    "        \n",
    "        assert results == {}, \"No results found\"\n",
    "        raise AssertionError('Error occurred during data fetch')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T15:34:36.521551Z",
     "start_time": "2024-07-08T15:34:36.516291Z"
    }
   },
   "id": "27cd93bef2469112",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Weaviate established\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'title': 'Detection Stays One Step Ahead of Deepfakes—for Now',\n 'content': 'In November, Intel announced its Real-Time Deepfake Detector, a platform for analyzing videos. (The term “deepfake” derives from the use of deep learning—an\\xa0...'}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_weaviate('Generating audio using deep learning models')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T15:41:43.818062Z",
     "start_time": "2024-07-08T15:41:38.796950Z"
    }
   },
   "id": "8ae03ede3b86d3ec",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embedding = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2', model_kwargs={'device':'cuda'})\n",
    "\n",
    "w_client = weaviate.connect_to_wcs(\n",
    "    cluster_url=WEAVIATE_URL, \n",
    "    auth_credentials=AuthApiKey(WEAVIATE_KEY), \n",
    "    additional_config=wvc.init.AdditionalConfig(\n",
    "        timeout=wvc.init.Timeout(init=360)\n",
    "    )\n",
    ")\n",
    "db = WeaviateVectorStore(client=w_client, embedding=hf_embedding, index_name='ResearchDocs', text_key='content')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:30:56.648099Z",
     "start_time": "2024-07-08T16:30:46.421218Z"
    }
   },
   "id": "10216ad45334f227",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "In November, Intel announced its Real-Time Deepfake Detector, a platform for analyzing videos. (The ...\n",
      "\n",
      "Document 2:\n",
      "DeepFake Detection. 158 papers with code • 8 benchmarks • 19 datasets. DeepFake Detection is the tas...\n",
      "\n",
      "Document 3:\n",
      "Detect Fakes is a research project that aims to counteract misinformation created by AI, such as Dee...\n",
      "\n",
      "Document 4:\n",
      "Detect DeepFakes: How to counteract misinformation created by AI\n",
      "Creative Commons\n",
      "Attribution 4.0 In...\n"
     ]
    }
   ],
   "source": [
    "query = \"tell me about deep learning\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# Print the first 100 characters of each result\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:100] + \"...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:34:32.789509Z",
     "start_time": "2024-07-08T16:34:31.105408Z"
    }
   },
   "id": "238b5a770b09e31b",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:37:44.126746Z",
     "start_time": "2024-07-08T16:37:44.124103Z"
    }
   },
   "id": "7f59dbe8af23158e",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:35.032117Z",
     "start_time": "2024-07-08T16:38:19.654416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb04d844d00f469abf3e2b0cb0b31607"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# model_id = 'google/gemma-2b-it'\n",
    "model_id = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, max_new_tokens=128, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T04:24:58.925391Z",
     "start_time": "2024-07-06T04:24:58.922208Z"
    }
   },
   "id": "bc9ab08c0a34c587",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "How to explain Internet for a medieval knight?<|end|>\n",
    "<|assistant|> \"\"\", return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')\n",
    "outputs = model.generate(**inputs, max_length=64)\n",
    "decoded = tokenizer.decode(outputs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T04:25:01.603858Z",
     "start_time": "2024-07-06T04:24:58.936936Z"
    }
   },
   "id": "68bf6fdfea537fa9",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"<|system|> You are a helpful assistant.<|end|><|user|> How to explain Internet for a medieval knight?<|end|><|assistant|> To explain the Internet to a medieval knight, you would need to use analogies and descriptions that relate to their world and understanding. Here's a possible explanation:\\n\\nImagine a vast network of\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T04:25:01.609483Z",
     "start_time": "2024-07-06T04:25:01.605747Z"
    }
   },
   "id": "b93858e4dc57a671",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arr matey, I be tellin' ye, a human be not built for eatin' helicopters, nor any other flying contraption for that matter. 'Tis a jest, I assure ye, for no soul should ever attempt such a feat. The only way to truly enjoy a helicopter is to admire its design and the skilled hands that craft it, not to consume it. So, I'd say the answer be zero, and I hope ye had a hearty laugh at the thought!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How many helicopters can a human eat in one sitting?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=128)[0]['generated_text'][-1]['content']\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:44.538055Z",
     "start_time": "2024-07-08T16:38:38.273320Z"
    }
   },
   "id": "d8e575e6a01db864",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'<|system|>\\nYou are a friendly chatbot who always responds in the style of a pirate<|end|>\\n<|user|>\\nHow many helicopters can a human eat in one sitting?<|end|>\\n<|endoftext|>'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-06T04:25:06.959394Z",
     "start_time": "2024-07-06T04:25:06.956242Z"
    }
   },
   "id": "ae62c5067351a1c4",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:44.552210Z",
     "start_time": "2024-07-08T16:38:44.539104Z"
    }
   },
   "id": "1d4e11ce502a1c69",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "user_turn = \"<|user|>\"\n",
    "system_turn = \"<|system|>\"\n",
    "assistant_turn = \"<|assistant|>\"\n",
    "end_turn = \"<|end|>\"\n",
    "\n",
    "### Prompt Template\n",
    "\n",
    "# General Prompt Template\n",
    "general_prompt_template = \"\"\"<|system|>\\n You are a helpful research assistant. You have to answer the questions \\\n",
    "by summarizing or deriving useful information from provided data. You must be careful and concise about the answers.<|end|>\n",
    "<|user|>\\n Here is the question:{question} and the provided data:{data}.<|end|>\n",
    "<|assistant|>\\n\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=general_prompt_template, input_variables=[\"question\", \"data\"])\n",
    "#PROMPT.format(system_turn=system_turn, user_turn=user_turn, assistant_turn=assistant_turn, end_turn=end_turn)\n",
    "\n",
    "# Chat Prompt Template\n",
    "chat_prompt_template = \"\"\"{system_turn}\\n You are a friendly chatbot who always responds in the style of a pirate.{end_turn}\n",
    "{user_turn}\\n {question} {assistant_turn}\\n\n",
    "\"\"\"\n",
    "chat_prompt = ChatPromptTemplate.from_template(template=chat_prompt_template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:47.561700Z",
     "start_time": "2024-07-08T16:38:47.557726Z"
    }
   },
   "id": "59319bd707068f07",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chain = PROMPT | llm | StrOutputParser() |  (lambda x: (x.split(\"<|assistant|>\")[-1]).strip())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:50.912695Z",
     "start_time": "2024-07-08T16:38:50.909251Z"
    }
   },
   "id": "d91a3c9a33269c15",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res = chain.invoke({'question': 'tell me about deep learning', 'data':retriever})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:57.738707Z",
     "start_time": "2024-07-08T16:38:51.644959Z"
    }
   },
   "id": "ac87d2cfd82b43c8",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'Deep learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data. It is particularly effective in tasks such as image and speech recognition, natural language processing, and predictive analytics.\\n\\nIn the provided data, there are two main components:\\n\\n1. WeaviateVectorStore: This is an object from the langchain_weaviate.vectorstores module, which likely represents a vector store that uses the Weaviate vector database to store and retrieve vector embeddings. Weaviate is a scalable, real-time graph database that can store,'"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T16:38:57.750423Z",
     "start_time": "2024-07-08T16:38:57.739713Z"
    }
   },
   "id": "8b839329e35fefd3",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d2b27b614d2f716"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
